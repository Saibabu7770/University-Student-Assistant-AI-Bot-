{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai babu\\AppData\\Local\\Temp\\ipykernel_41160\\2594512491.py:68: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
      "c:\\Users\\Sai babu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sai babu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading PDF documents from External_Database\n",
      "Found 3 PDF files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDF documents:   0%|                              | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDF documents: 100%|██████████████████████| 3/3 [00:00<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Loading file: External_Database\\Job_description.pdf\n",
      "Finished loading file: External_Database\\Job_description.pdf\n",
      "Loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Finished loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Loaded 14 PDF documents from External_Database\n",
      "Split into 71 chunks of text (max. 300 tokens each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 71 embeddings and texts to embeddings_output.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Define the folder for storing the database\n",
    "PERSIST_DIRECTORY = os.environ.get('PERSIST_DIRECTORY', 'db')\n",
    "\n",
    "# Define the Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "    persist_directory=PERSIST_DIRECTORY,\n",
    "    anonymized_telemetry=False\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "source_directory = os.environ.get('SOURCE_DIRECTORY', 'External_Database')\n",
    "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME', 'all-MiniLM-L6-v2')\n",
    "\n",
    "# Optimized parameters for text splitting\n",
    "chunk_size = 300\n",
    "chunk_overlap = 30\n",
    "max_files_to_process = 5  # Limit for debugging\n",
    "\n",
    "# Function to load a single PDF document\n",
    "def load_single_pdf(file_path: str) -> List[Document]:\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    print(f\"Finished loading file: {file_path}\")\n",
    "    return documents\n",
    "\n",
    "# Function to load all PDF documents from the source directory\n",
    "def load_pdf_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    all_pdf_files = glob.glob(os.path.join(source_dir, \"**/*.pdf\"), recursive=True)[:max_files_to_process]\n",
    "    filtered_files = [file_path for file_path in all_pdf_files if file_path not in ignored_files]\n",
    "\n",
    "    print(f\"Found {len(filtered_files)} PDF files to process.\")\n",
    "\n",
    "    results = []\n",
    "    for file_path in tqdm(filtered_files, desc='Loading PDF documents', ncols=80):\n",
    "        results.extend(load_single_pdf(file_path))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to process and split documents into text chunks\n",
    "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
    "    print(f\"Loading PDF documents from {source_directory}\")\n",
    "    documents = load_pdf_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    print(f\"Loaded {len(documents)} PDF documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return texts\n",
    "\n",
    "# Function to generate embeddings and save them along with the text content\n",
    "def generate_and_save_embeddings(file_path: str):\n",
    "    # Load the embeddings model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "    \n",
    "    # Process your documents to get the text chunks\n",
    "    texts = process_documents()\n",
    "    text_contents = [text.page_content for text in texts]\n",
    "    \n",
    "    # Generate embeddings for the text contents\n",
    "    embeddings_list = embeddings.embed_documents(text_contents)\n",
    "    \n",
    "    # Create a data structure to save both embeddings and text\n",
    "    data_to_save = [\n",
    "        {\"embedding\": embedding, \"text\": text}\n",
    "        for embedding, text in zip(embeddings_list, text_contents)\n",
    "    ]\n",
    "    \n",
    "    # Save the data to a JSON file\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(data_to_save, file)\n",
    "    \n",
    "    print(f\"Saved {len(data_to_save)} embeddings and texts to {file_path}\")\n",
    "\n",
    "# Specify the file path for saving the embeddings\n",
    "file_path = \"embeddings_output.json\"\n",
    "\n",
    "# Call the function to generate and save embeddings\n",
    "generate_and_save_embeddings(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store\n",
      "Loading PDF documents from External_Database\n",
      "Found 3 PDF files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDF documents: 100%|██████████████████████| 3/3 [00:00<00:00, 37.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Finished loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Loading file: External_Database\\Job_description.pdf\n",
      "Finished loading file: External_Database\\Job_description.pdf\n",
      "Loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Finished loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Loaded 14 PDF documents from External_Database\n",
      "Split into 71 chunks of text (max. 300 tokens each)\n",
      "Sample extracted text chunk: credit-hour program. By the time you graduate, your toolbox will allow you to pick the best approach\n",
      "to solve the problems you face.\n",
      "Career Possibilities\n",
      "If it seems like you're seeing a lot of job postings for data scientists and business intelligence analysts,\n",
      "Creating embeddings. May take some minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Sai babu\\AppData\\Local\\Temp\\ipykernel_41160\\2164698747.py:97: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import List\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Define the folder for storing the database\n",
    "PERSIST_DIRECTORY = os.environ.get('PERSIST_DIRECTORY', 'db')\n",
    "\n",
    "# Define the Chroma settings\n",
    "CHROMA_SETTINGS = Settings(\n",
    "    persist_directory=PERSIST_DIRECTORY,\n",
    "    anonymized_telemetry=False\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "source_directory = os.environ.get('SOURCE_DIRECTORY', 'External_Database')\n",
    "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME', 'all-MiniLM-L6-v2')\n",
    "\n",
    "# Optimized parameters for debugging\n",
    "chunk_size = 300\n",
    "chunk_overlap = 30\n",
    "max_files_to_process = 5  # Only process a few files for testing\n",
    "\n",
    "# Function to check if a vector store exists\n",
    "def does_vectorstore_exist(persist_directory: str) -> bool:\n",
    "    if os.path.exists(os.path.join(persist_directory, 'index')):\n",
    "        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
    "            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n",
    "            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n",
    "            if len(list_index_files) > 3:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to load a single PDF document with debug info\n",
    "def load_single_pdf(file_path: str) -> List[Document]:\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    print(f\"Finished loading file: {file_path}\")\n",
    "    return documents\n",
    "\n",
    "# Function to load all PDF documents from the source directory with debug info\n",
    "def load_pdf_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    all_pdf_files = glob.glob(os.path.join(source_dir, \"**/*.pdf\"), recursive=True)[:max_files_to_process]\n",
    "    filtered_files = [file_path for file_path in all_pdf_files if file_path not in ignored_files]\n",
    "\n",
    "    print(f\"Found {len(filtered_files)} PDF files to process.\")\n",
    "\n",
    "    # Temporarily disable multiprocessing for debugging\n",
    "    results = []\n",
    "    for file_path in tqdm(filtered_files, desc='Loading PDF documents', ncols=80):\n",
    "        results.extend(load_single_pdf(file_path))  # Direct call without multiprocessing\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to process and split documents into chunks\n",
    "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
    "    print(f\"Loading PDF documents from {source_directory}\")\n",
    "    documents = load_pdf_documents(source_directory, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    print(f\"Loaded {len(documents)} PDF documents from {source_directory}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return texts\n",
    "\n",
    "# Main function to create or update the vector store\n",
    "def main():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "    if does_vectorstore_exist(PERSIST_DIRECTORY):\n",
    "        print(f\"Appending to existing vector store at {PERSIST_DIRECTORY}\")\n",
    "        db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "        collection = db.get()\n",
    "        texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
    "    else:\n",
    "        print(\"Creating new vector store\")\n",
    "        texts = process_documents()\n",
    "    \n",
    "    # Print a random chunk of text\n",
    "    import random\n",
    "    if texts:\n",
    "        random_chunk = random.choice(texts)\n",
    "        print(\"Sample extracted text chunk:\", random_chunk.page_content)\n",
    "    \n",
    "    print(f\"Creating embeddings. May take some minutes...\")\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=PERSIST_DIRECTORY)\n",
    "    db.persist()\n",
    "    db = None\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sai babu\\AppData\\Local\\Temp\\ipykernel_41160\\2543622340.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF documents from External_Database\n",
      "Found 3 PDF files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDF documents: 100%|██████████████████████| 3/3 [00:00<00:00, 31.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Finished loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Loading file: External_Database\\Job_description.pdf\n",
      "Finished loading file: External_Database\\Job_description.pdf\n",
      "Loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Finished loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Loaded 14 PDF documents from External_Database\n",
      "Split into 71 chunks of text (max. 300 tokens each)\n",
      "Embedding 1: [0.04865654557943344, 0.03969587758183479, -0.04392899572849274, 0.04689142480492592, -0.03219174966216087, -0.02440021000802517, 0.07106217741966248, -0.016942642629146576, -0.07020010054111481, 0.012026670388877392]...\n",
      "Embedding 2: [-0.02117951214313507, -0.0628172755241394, 0.05858604982495308, -0.0014558365801349282, 0.006836882792413235, -0.037297822535037994, -0.016918009147047997, 0.10872379690408707, -0.10490531474351883, 0.07856155186891556]...\n",
      "Embedding 3: [-0.020607495680451393, 0.01029456127434969, 0.009476795792579651, 0.055719681084156036, 0.0006447816849686205, -0.053860221058130264, -0.03251653537154198, 0.02800251543521881, -0.02673499658703804, 0.03464403748512268]...\n",
      "Embedding 4: [-0.030184490606188774, -0.038931507617235184, 0.02843565307557583, 0.06997009366750717, 0.01904807612299919, -0.09459130465984344, -0.0783746987581253, 0.0028394702821969986, -0.0036320751532912254, 0.03443096578121185]...\n",
      "Embedding 5: [0.018685881048440933, 0.0006622286746278405, 0.05653810501098633, 0.025455165654420853, 0.06255599111318588, -0.06566297262907028, 0.002896373625844717, 0.019843237474560738, -0.08486015349626541, 0.10008672624826431]...\n"
     ]
    }
   ],
   "source": [
    "# Chunk to show the embeddings\n",
    "def show_embeddings():\n",
    "    # Load the embeddings model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "    # Load the vector store\n",
    "    db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "    \n",
    "    # Fetch some documents and extract the text content\n",
    "    texts = process_documents()  # Ensure this function provides the chunks you want\n",
    "    text_contents = [text.page_content for text in texts]\n",
    "\n",
    "    # Generate embeddings for the text contents\n",
    "    embeddings_list = embeddings.embed_documents(text_contents)\n",
    "\n",
    "    # Display the first few embeddings for inspection\n",
    "    for i, embedding in enumerate(embeddings_list[:5]):\n",
    "        print(f\"Embedding {i + 1}: {embedding[:10]}...\")  # Print the first 10 values for brevity\n",
    "\n",
    "# Call the function to show the embeddings\n",
    "show_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF documents from External_Database\n",
      "Found 3 PDF files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDF documents: 100%|██████████████████████| 3/3 [00:00<00:00, 28.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Finished loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Loading file: External_Database\\Job_description.pdf\n",
      "Finished loading file: External_Database\\Job_description.pdf\n",
      "Loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Finished loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Loaded 14 PDF documents from External_Database\n",
      "Split into 71 chunks of text (max. 300 tokens each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and text content saved to embeddings_with_text.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Chunk to save the embeddings with their associated text content to a file\n",
    "def save_embeddings_to_file(filename=\"embeddings_with_text.json\"):\n",
    "    # Load the embeddings model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "    # Load the vector store\n",
    "    db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "    \n",
    "    # Fetch some documents and extract the text content\n",
    "    texts = process_documents()  # Ensure this function provides the chunks you want\n",
    "    text_contents = [text.page_content for text in texts]\n",
    "\n",
    "    # Generate embeddings for the text contents\n",
    "    embeddings_list = embeddings.embed_documents(text_contents)\n",
    "\n",
    "    # Create a list of dictionaries to store embeddings and text content\n",
    "    embeddings_with_text = [\n",
    "        {\"embedding\": embedding, \"text\": text}\n",
    "        for embedding, text in zip(embeddings_list, text_contents)\n",
    "    ]\n",
    "\n",
    "    # Save to a JSON file\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(embeddings_with_text, file)\n",
    "    print(f\"Embeddings and text content saved to {filename}\")\n",
    "\n",
    "# Call the function to save the embeddings to a file\n",
    "save_embeddings_to_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF documents from External_Database\n",
      "Found 3 PDF files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDF documents: 100%|██████████████████████| 3/3 [00:00<00:00, 24.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Finished loading file: External_Database\\clarkson_edu_academics_majors_minors_data_science_grad.pdf\n",
      "Loading file: External_Database\\Job_description.pdf\n",
      "Finished loading file: External_Database\\Job_description.pdf\n",
      "Loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Finished loading file: External_Database\\Student NL Sept 19, 2024.pdf\n",
      "Loaded 14 PDF documents from External_Database\n",
      "Split into 71 chunks of text (max. 300 tokens each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1: [0.04865654557943344, 0.03969587758183479, -0.04392899572849274, 0.04689142480492592, -0.03219174966216087, -0.02440021000802517, 0.07106217741966248, -0.016942642629146576, -0.07020010054111481, 0.012026670388877392]...\n",
      "Embedding 2: [-0.02117951214313507, -0.0628172755241394, 0.05858604982495308, -0.0014558365801349282, 0.006836882792413235, -0.037297822535037994, -0.016918009147047997, 0.10872379690408707, -0.10490531474351883, 0.07856155186891556]...\n",
      "Embedding 3: [-0.020607495680451393, 0.01029456127434969, 0.009476795792579651, 0.055719681084156036, 0.0006447816849686205, -0.053860221058130264, -0.03251653537154198, 0.02800251543521881, -0.02673499658703804, 0.03464403748512268]...\n",
      "Embedding 4: [-0.030184490606188774, -0.038931507617235184, 0.02843565307557583, 0.06997009366750717, 0.01904807612299919, -0.09459130465984344, -0.0783746987581253, 0.0028394702821969986, -0.0036320751532912254, 0.03443096578121185]...\n",
      "Embedding 5: [0.018685881048440933, 0.0006622286746278405, 0.05653810501098633, 0.025455165654420853, 0.06255599111318588, -0.06566297262907028, 0.002896373625844717, 0.019843237474560738, -0.08486015349626541, 0.10008672624826431]...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def show_embeddings_and_save():\n",
    "    # Load the embeddings model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "    # Load the vector store\n",
    "    db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "    \n",
    "    # Fetch some documents and extract the text content\n",
    "    texts = process_documents()  # Ensure this function provides the chunks you want\n",
    "    text_contents = [text.page_content for text in texts]\n",
    "\n",
    "    # Generate embeddings for the text contents\n",
    "    embeddings_list = embeddings.embed_documents(text_contents)\n",
    "\n",
    "    # Save the embeddings to a file\n",
    "    with open(\"embeddings_output.json\", \"w\") as file:\n",
    "        json.dump(embeddings_list, file)\n",
    "    \n",
    "    # Display the first few embeddings for inspection\n",
    "    for i, embedding in enumerate(embeddings_list[:5]):\n",
    "        print(f\"Embedding {i + 1}: {embedding[:10]}...\")  # Print the first 10 values for brevity\n",
    "\n",
    "# Call the function to show and save the embeddings\n",
    "show_embeddings_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make an appointment for the International Center, you can follow these steps:\n",
      "\n",
      "1. Go to Handshake: The International Center uses Handshake, a career and internship platform, to schedule appointments. You can access Handshake through the Clarkson University website or by searching for \"Handshake\" in your university's portal.\n",
      "2. Search for Appointments: In Handshake, search for \"International Center\" or \"Study Abroad\" to find available appointments.\n",
      "3. Choose an Appointment Type: Select the type of appointment you want to schedule, such as a one-on-one appointment or a drop-in hour.\n",
      "4. Choose a Date and Time: Select a date and time that works for you from the available options.\n",
      "5. Fill Out the Appointment Form: Fill out the appointment form with your name, email, and any additional information requested.\n",
      "6. Confirm Your Appointment: Once you've submitted the appointment form, you'll receive a confirmation email with the details of your appointment.\n",
      "\n",
      "Alternatively, you can also schedule an appointment by:\n",
      "\n",
      "1. Emailing the International Center: You can email the International Center at internationalcenter@clarkson.edu to request an appointment. Be sure to include your name, student ID, and a brief description of what you'd like to discuss during the appointment.\n",
      "2. Visiting the International Center in Person: You can also stop by the International Center in person to schedule an appointment. The center is located in ERC 2300, and you can speak with a staff member to schedule a time that works for you.\n",
      "\n",
      "Remember to check-in when you arrive for your appointment or drop-in hour.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load the embeddings and text content from the JSON file\n",
    "def load_embeddings_from_file(filename=\"embeddings_with_text.json\"):\n",
    "    with open(filename, \"r\") as file:\n",
    "        embeddings_with_text = json.load(file)\n",
    "    return embeddings_with_text\n",
    "\n",
    "# Function to retrieve the top relevant text chunks\n",
    "def retrieve_relevant_text(sample_query, embeddings_with_text, embeddings_model):\n",
    "    # Generate embedding for the sample query\n",
    "    query_embedding = embeddings_model.embed_documents([sample_query])[0]\n",
    "\n",
    "    # Extract embeddings and text content from the loaded data\n",
    "    all_embeddings = [item[\"embedding\"] for item in embeddings_with_text]\n",
    "    all_texts = [item[\"text\"] for item in embeddings_with_text]\n",
    "\n",
    "    # Compute cosine similarity between the query embedding and all stored embeddings\n",
    "    similarities = cosine_similarity([query_embedding], all_embeddings)[0]\n",
    "\n",
    "    # Find the indices of the top 3 most similar text chunks\n",
    "    top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "\n",
    "    # Collect the top relevant text chunks\n",
    "    relevant_texts = [all_texts[index] for index in top_indices]\n",
    "    return relevant_texts\n",
    "\n",
    "# Load the embeddings and text content\n",
    "embeddings_with_text = load_embeddings_from_file()\n",
    "\n",
    "# Load your embeddings model\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "# Initialize your LLM\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key='',\n",
    "    model_name=\"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "# Sample query\n",
    "sample_query = \"how to make appoitment for international center\"\n",
    "\n",
    "# Retrieve relevant text chunks using embeddings\n",
    "relevant_text_chunks = retrieve_relevant_text(sample_query, embeddings_with_text, embeddings_model)\n",
    "\n",
    "# Format the context to pass to the LLM\n",
    "context = \"\\n\".join(relevant_text_chunks)\n",
    "\n",
    "# Use the LLM with the retrieved context\n",
    "response = llm.invoke(f\"Context: {context}\\n\\nQuestion: {sample_query}\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boris Jukic is the Director of Business Analytics at Clarkson University.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load the embeddings and text content from the JSON file\n",
    "def load_embeddings_from_file(filename=\"embeddings_with_text.json\"):\n",
    "    with open(filename, \"r\") as file:\n",
    "        embeddings_with_text = json.load(file)\n",
    "    return embeddings_with_text\n",
    "\n",
    "# Function to retrieve the top relevant text chunks\n",
    "def retrieve_relevant_text(sample_query, embeddings_with_text, embeddings_model):\n",
    "    # Generate embedding for the sample query\n",
    "    query_embedding = embeddings_model.embed_documents([sample_query])[0]\n",
    "\n",
    "    # Extract embeddings and text content from the loaded data\n",
    "    all_embeddings = [item[\"embedding\"] for item in embeddings_with_text]\n",
    "    all_texts = [item[\"text\"] for item in embeddings_with_text]\n",
    "\n",
    "    # Compute cosine similarity between the query embedding and all stored embeddings\n",
    "    similarities = cosine_similarity([query_embedding], all_embeddings)[0]\n",
    "\n",
    "    # Find the indices of the top 3 most similar text chunks\n",
    "    top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "\n",
    "    # Collect the top relevant text chunks\n",
    "    relevant_texts = [all_texts[index] for index in top_indices]\n",
    "    return relevant_texts\n",
    "\n",
    "# Load the embeddings and text content\n",
    "embeddings_with_text = load_embeddings_from_file()\n",
    "\n",
    "# Load your embeddings model\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "# Initialize your LLM\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key='..',\n",
    "    model_name=\"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "# Sample query\n",
    "sample_query = \"who is boris?\"\n",
    "\n",
    "# Retrieve relevant text chunks using embeddings\n",
    "relevant_text_chunks = retrieve_relevant_text(sample_query, embeddings_with_text, embeddings_model)\n",
    "\n",
    "# Format the context to pass to the LLM\n",
    "context = \"\\n\".join(relevant_text_chunks)\n",
    "\n",
    "# Use the LLM with the retrieved context\n",
    "response = llm.invoke(f\"Context: {context}\\n\\nQuestion: {sample_query}\")\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
